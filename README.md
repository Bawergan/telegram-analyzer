# Telegram Chat Analyzer

Это мощный ETL-конвейер (Extract, Transform, Load) для обработки архивов экспорта чатов Telegram. Он предназначен для надежного извлечения данных из `result.json`, их структурирования, валидации и загрузки в базу данных SQLite. Система эффективно обрабатывает текстовые сообщения, медиафайлы и реакции, реализуя умную логику слияния (Upsert) для поддержания данных в актуальном состоянии.

Проект является фундаментом для дальнейшей интеграции с AI-моделями с целью анализа контента, генерации описаний и создания векторных представлений для семантического поиска.

## Ключевые возможности

*   **Надежная валидация данных:** Использование `Pydantic` для строгой проверки структуры входящих JSON-файлов перед обработкой.
*   **Эффективная логика слияния (Upsert):** Скрипт не просто загружает данные, а интеллектуально их сливает:
    *   **Добавляет** новые сообщения.
    *   **Обновляет** сообщения, которые были отредактированы в Telegram (на основе поля `edited`).
    *   **Пропускает** уже существующие и неизмененные сообщения, обеспечивая идемпотентность.
*   **Централизованное управление медиафайлами:** Автоматически копирует все медиафайлы (фото, видео, голосовые сообщения) из разрозненных папок экспорта в единое централизованное хранилище с уникальными именами.
*   **Пакетная обработка:** Управляющий скрипт `run_importer.py` позволяет одной командой обработать неограниченное количество экспортов чатов, рекурсивно находя их в указанной директории.
*   **Структурированная база данных:** Использование `SQLAlchemy` для создания реляционной схемы данных (чаты, сообщения, пользователи, реакции), что упрощает дальнейший анализ и запросы.
*   **Высокое качество кода:** Весь основной функционал покрыт Unit-тестами с использованием `pytest` и `unittest.mock`.

## Структура проекта

```
/telegram-analyzer
├── media_storage/          # -> Централизованное хранилище для всех медиафайлов (создается автоматически)
├── .env                    # -> Ваш локальный файл конфигурации (см. ниже)
├── .env.example            # -> Пример файла конфигурации
├── database.py             # -> Ядро: схемы Pydantic, модели SQLAlchemy, основная логика ETL
├── run_importer.py         # -> Точка входа: скрипт для запуска пакетного импорта
├── requirements.txt        # -> Зависимости проекта
├── test_database.py        # -> Unit-тесты для database.py
└── test_run_importer.py    # -> Unit-тесты для run_importer.py
```

## Установка и настройка

**Шаг 1: Клонирование репозитория**
```bash
git clone <your-repository-url>
cd telegram-analyzer
```

**Шаг 2: Создание и активация виртуального окружения**
```bash
# Для Windows
python -m venv venv
.\venv\Scripts\activate

# Для macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

**Шаг 3: Установка зависимостей**
```bash
pip install -r requirements.txt
```

**Шаг 4: Настройка конфигурации**
Создайте файл `.env` в корне проекта, скопировав `.env.example`:
```bash
# Windows
copy .env.example .env

# macOS/Linux
cp .env.example .env
```
Файл `.env` по умолчанию уже настроен для работы с SQLite и хранилищем `media_storage` в папке проекта. Вы можете изменить пути при необходимости.

```dotenv
# .env
DATABASE_URL="sqlite:///telegram_archive.db"
MEDIA_STORAGE_ROOT="media_storage"
```

## Использование

**Шаг 1: Подготовка данных**
Создайте корневую папку (например, `telegram_json_exports`) и поместите в нее папки с вашими экспортами из Telegram. Скрипт ожидает, что в каждой такой папке будет файл `result.json` и сопутствующие медиафайлы.

Пример структуры ваших данных:
```
/telegram_json_exports/
├── chat_with_friend/
│   ├── photos/
│   │   └── photo_123.jpg
│   └── result.json
├── work_group_chat/
│   ├── files/
│   │   └── important_doc.pdf
│   └── result.json
└── ...
```

**Шаг 2: Запуск импорта**
Выполните команду в терминале, указав путь к вашей корневой папке с экспортами (`telegram_json_exports`):

```bash
python run_importer.py --source /path/to/telegram_json_exports
```

Скрипт начнет обработку. Вы увидите логи в консоли, информирующие о прогрессе. После завершения:
*   В корне проекта появится файл базы данных `telegram_archive.db`.
*   Папка `media_storage` будет наполнена всеми медиафайлами из ваших чатов, переименованными по шаблону `{chat_id}_{message_id}.ext`.

## Архитектура

1.  **Поиск (`run_importer.py`):** Управляющий скрипт рекурсивно ищет все файлы `result.json` в указанной директории `--source`.
2.  **Валидация (`database.py`):** Содержимое каждого `result.json` загружается и передается в `TelegramChatExportSchema` (Pydantic). Если структура JSON корректна, Pydantic преобразует его в строго типизированные Python-объекты. В противном случае процесс для этого файла прерывается с ошибкой.
3.  **Загрузка и слияние (`load_validated_data`):**
    *   Для каждого чата все существующие сообщения из БД загружаются в словарь (`dict`) для мгновенного доступа по ID (сложность O(1)).
    *   Скрипт итерируется по сообщениям из валидированного JSON-файла.
    *   **Если сообщение уже есть в словаре:**
        *   Проверяется дата редактирования. Если дата в JSON новее, чем в БД, запись в БД обновляется (текст, реакции и т.д.).
        *   В противном случае сообщение пропускается.
    *   **Если сообщения нет в словаре:**
        *   Создается новый объект `Message` для добавления в БД.
4.  **Обработка медиафайлов (`_populate_message_from_schema`):**
    *   Для каждого сообщения, содержащего поле `file`, скрипт формирует путь к исходному файлу.
    *   Создается новое, уникальное имя файла.
    *   Файл копируется в центральное хранилище (`media_storage`).
    *   Относительный путь к новому файлу сохраняется в поле `stored_media_path` в базе данных.
5.  **Сохранение в БД:** Новые и обновленные объекты коммитятся в базу данных с помощью `SQLAlchemy`.

## Тестирование

Проект имеет высокий уровень тестового покрытия. Для запуска всех тестов выполните одну команду:
```bash
pytest
```

## Дальнейшие планы

Этот проект является основой для более сложных систем анализа данных. Следующие шаги:

*   **[В РАЗРАБОТКЕ] Этап 2: AI-обработка контента (`ai_processors.py`)**
    *   **Image-to-Text:** Генерация текстовых описаний для изображений с помощью моделей Image Captioning (например, `BLIP`).
    *   **Speech-to-Text:** Транскрибация голосовых сообщений и видео с помощью моделей (например, `OpenAI Whisper`).
    *   **Struct-to-Text:** Преобразование структурированных сообщений (опросы, геолокации) в осмысленный текст.

*   **[ПЛАНИРУЕТСЯ] Этап 3: Векторные представления (Embeddings)**
    *   Создание векторных представлений (embeddings) для всего текстового контента (оригинального и сгенерированного AI).
    *   Сохранение векторов в БД для реализации семантического поиска.

*   **[БУДУЩЕЕ] Этап 4: Интерфейс для поиска и анализа**
    *   Создание API (например, на FastAPI) для выполнения семантического поиска по базе данных.
    *   Разработка простого UI (например, на Streamlit) для взаимодействия с данными.

---